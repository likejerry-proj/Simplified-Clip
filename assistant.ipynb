{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor _ in range(1849):\\n    a = next(data_iter)\\n    if(_ % 100 == 0):\\n        print(_)\\nprint(a[0].shape)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from dataloader.dataset import CLIP_COCO_dataset\n",
    "from dataloader.data_loaders import get_dataloader\n",
    "\n",
    "from model.model import CLIP\n",
    "from utils.simple_tokenizer import SimpleTokenizer\n",
    "from utils.custom_schedulers import get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from utils import set_seed, mkdir, setup_logger, load_config_file\n",
    "\n",
    "from torch.optim import Adam, AdamW # both are same but AdamW has a default weight decay\n",
    "\n",
    "import argparse\n",
    "\n",
    "DATA_CONFIG_PATH = 'dataloader/data_config.yaml'\n",
    "TRAINER_CONFIG_PATH = 'trainer/train_config.yaml'\n",
    "MODEL_CONFIG_PATH = 'model/model_config.yaml'\n",
    "\n",
    "data_config = load_config_file(DATA_CONFIG_PATH)\n",
    "train_config = load_config_file(TRAINER_CONFIG_PATH)\n",
    "model_config = load_config_file(MODEL_CONFIG_PATH)\n",
    "config = OmegaConf.merge(train_config, data_config)\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "train_dataset = CLIP_COCO_dataset(config, tokenizer)\n",
    "\n",
    "train_dataloader = get_dataloader(config, train_dataset, is_train=True)\n",
    "\n",
    "# dataloader的size\n",
    "data_iter = iter(train_dataloader)\n",
    "\n",
    "# 查看dataloader的维度\n",
    "\n",
    "#print(len(train_dataloader))\n",
    "\n",
    "\n",
    "a = next(data_iter)\n",
    "#print(a[1][0])\n",
    "#print(a[1][1])\n",
    "\n",
    "\"\"\"\n",
    "for _ in range(1849):\n",
    "    a = next(data_iter)\n",
    "    if(_ % 100 == 0):\n",
    "        print(_)\n",
    "print(a[0].shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with one image-text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 1024, 'image_resolution': 224, 'vision_layers': (3, 4, 6, 3), 'vision_width': 64, 'vision_patch_size': None, 'context_length': 77, 'vocab_size': 49408, 'transformer_width': 512, 'transformer_heads': 8, 'transformer_layers': 6}\n",
      "x.shape1: torch.Size([64, 77, 512])\n",
      "y.shape: torch.Size([64, 512])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63])\n",
      "x.shape2: torch.Size([64, 1024])\n",
      "input_images: torch.Size([64, 3, 224, 224]) \n",
      " image_features: torch.Size([64, 1024]) \n",
      " \n",
      "input_texts: torch.Size([64, 77]) \n",
      " text_features: torch.Size([64, 1024])\n"
     ]
    }
   ],
   "source": [
    "batch = a\n",
    "\n",
    "input_images, input_texts = batch\n",
    "\n",
    "#config.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#input_images = input_images.to(torch.device(config.device))\n",
    "#input_texts = input_texts.to(torch.device(config.device))\n",
    "\n",
    "\n",
    "\n",
    "model_params = dict(model_config.RN50)\n",
    "model_params['vision_layers'] = tuple(model_params['vision_layers'])\n",
    "model_params['vision_patch_size'] = None\n",
    "model = CLIP(**model_params)\n",
    "print(model_params)\n",
    "\n",
    "#model = model.to(torch.device(config.device))\n",
    "#model.train()\n",
    "\n",
    "image_features, text_features = model(input_images, input_texts)\n",
    "\n",
    "print('input_images:',input_images.shape,'\\n', 'image_features:', image_features.shape, '\\n','\\n'\n",
    "        'input_texts:',input_texts.shape, '\\n','text_features:',text_features.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 17, 11, 12, 14, 12, 11, 18, 10, 22, 11, 12, 10, 10, 13, 14, 11, 15,\n",
      "        10, 13, 12, 12, 12, 10, 12, 14, 12, 10, 12, 11, 11, 13, 12, 11, 11, 10,\n",
      "        10, 15, 18, 12, 12, 10, 13, 14, 16, 12, 16, 13, 14, 14, 11, 11, 14, 14,\n",
      "        14, 10, 22, 11, 10,  9, 12, 14, 11, 15])\n",
      "tensor([49406,   320,  4038,   539,  2254, 22611,  2374,   530,   518, 11795,\n",
      "          530,   911, 11747,   536,   320,  7147,   269, 49407,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([48128, 48129, 48130, 48131, 48132, 48133, 48134, 48135, 48136, 48137,\n",
      "        48138, 48139, 48140, 48141, 48142, 48143, 48144, 48145, 48146, 48147,\n",
      "        48148, 48149, 48150, 48151, 48152, 48153, 48154, 48155, 48156, 48157,\n",
      "        48158, 48159, 48160, 48161, 48162, 48163, 48164, 48165, 48166, 48167,\n",
      "        48168, 48169, 48170, 48171, 48172, 48173, 48174, 48175, 48176, 48177,\n",
      "        48178, 48179, 48180, 48181, 48182, 48183, 48184, 48185, 48186, 48187,\n",
      "        48188, 48189, 48190, 48191, 48192, 48193, 48194, 48195, 48196, 48197,\n",
      "        48198, 48199, 48200, 48201, 48202, 48203, 48204, 48205, 48206, 48207,\n",
      "        48208, 48209, 48210, 48211, 48212, 48213, 48214, 48215, 48216, 48217,\n",
      "        48218, 48219, 48220, 48221, 48222, 48223, 48224, 48225, 48226, 48227,\n",
      "        48228, 48229, 48230, 48231, 48232, 48233, 48234, 48235, 48236, 48237,\n",
      "        48238, 48239, 48240, 48241, 48242, 48243, 48244, 48245, 48246, 48247,\n",
      "        48248, 48249, 48250, 48251, 48252, 48253, 48254, 48255, 48256, 48257,\n",
      "        48258, 48259, 48260, 48261, 48262, 48263, 48264, 48265, 48266, 48267,\n",
      "        48268, 48269, 48270, 48271, 48272, 48273, 48274, 48275, 48276, 48277,\n",
      "        48278, 48279, 48280, 48281, 48282, 48283, 48284, 48285, 48286, 48287,\n",
      "        48288, 48289, 48290, 48291, 48292, 48293, 48294, 48295, 48296, 48297,\n",
      "        48298, 48299, 48300, 48301, 48302, 48303, 48304, 48305, 48306, 48307,\n",
      "        48308, 48309, 48310, 48311, 48312, 48313, 48314, 48315, 48316, 48317,\n",
      "        48318, 48319, 48320, 48321, 48322, 48323, 48324, 48325, 48326, 48327,\n",
      "        48328, 48329, 48330, 48331, 48332, 48333, 48334, 48335, 48336, 48337,\n",
      "        48338, 48339, 48340, 48341, 48342, 48343, 48344, 48345, 48346, 48347,\n",
      "        48348, 48349, 48350, 48351, 48352, 48353, 48354, 48355, 48356, 48357,\n",
      "        48358, 48359, 48360, 48361, 48362, 48363, 48364, 48365, 48366, 48367,\n",
      "        48368, 48369, 48370, 48371, 48372, 48373, 48374, 48375, 48376, 48377,\n",
      "        48378, 48379, 48380, 48381, 48382, 48383, 48384, 48385, 48386, 48387,\n",
      "        48388, 48389, 48390, 48391, 48392, 48393, 48394, 48395, 48396, 48397,\n",
      "        48398, 48399, 48400, 48401, 48402, 48403, 48404, 48405, 48406, 48407,\n",
      "        48408, 48409, 48410, 48411, 48412, 48413, 48414, 48415, 48416, 48417,\n",
      "        48418, 48419, 48420, 48421, 48422, 48423, 48424, 48425, 48426, 48427,\n",
      "        48428, 48429, 48430, 48431, 48432, 48433, 48434, 48435, 48436, 48437,\n",
      "        48438, 48439, 48440, 48441, 48442, 48443, 48444, 48445, 48446, 48447,\n",
      "        48448, 48449, 48450, 48451, 48452, 48453, 48454, 48455, 48456, 48457,\n",
      "        48458, 48459, 48460, 48461, 48462, 48463, 48464, 48465, 48466, 48467,\n",
      "        48468, 48469, 48470, 48471, 48472, 48473, 48474, 48475, 48476, 48477,\n",
      "        48478, 48479, 48480, 48481, 48482, 48483, 48484, 48485, 48486, 48487,\n",
      "        48488, 48489, 48490, 48491, 48492, 48493, 48494, 48495, 48496, 48497,\n",
      "        48498, 48499, 48500, 48501, 48502, 48503, 48504, 48505, 48506, 48507,\n",
      "        48508, 48509, 48510, 48511, 48512, 48513, 48514, 48515, 48516, 48517,\n",
      "        48518, 48519, 48520, 48521, 48522, 48523, 48524, 48525, 48526, 48527,\n",
      "        48528, 48529, 48530, 48531, 48532, 48533, 48534, 48535, 48536, 48537,\n",
      "        48538, 48539, 48540, 48541, 48542, 48543, 48544, 48545, 48546, 48547,\n",
      "        48548, 48549, 48550, 48551, 48552, 48553, 48554, 48555, 48556, 48557,\n",
      "        48558, 48559, 48560, 48561, 48562, 48563, 48564, 48565, 48566, 48567,\n",
      "        48568, 48569, 48570, 48571, 48572, 48573, 48574, 48575, 48576, 48577,\n",
      "        48578, 48579, 48580, 48581, 48582, 48583, 48584, 48585, 48586, 48587,\n",
      "        48588, 48589, 48590, 48591, 48592, 48593, 48594, 48595, 48596, 48597,\n",
      "        48598, 48599, 48600, 48601, 48602, 48603, 48604, 48605, 48606, 48607,\n",
      "        48608, 48609, 48610, 48611, 48612, 48613, 48614, 48615, 48616, 48617,\n",
      "        48618, 48619, 48620, 48621, 48622, 48623, 48624, 48625, 48626, 48627,\n",
      "        48628, 48629, 48630, 48631, 48632, 48633, 48634, 48635, 48636, 48637,\n",
      "        48638, 48639])\n"
     ]
    }
   ],
   "source": [
    "print(input_texts.argmax(dim=-1))\n",
    "print(input_texts[1])\n",
    "#print(torch.arange(64))\n",
    "\n",
    "test = torch.arange(2523136).reshape([64,77,512])\n",
    "test = test[torch.arange(64), input_texts.argmax(dim=-1)]\n",
    "print(test[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-T",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
